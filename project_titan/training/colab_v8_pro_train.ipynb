{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a154983",
   "metadata": {},
   "source": [
    "# Project Titan ‚Äî Training v8 PRO (Zero-Error Edition)\n",
    "\n",
    "Notebook de treinamento **production-grade** para o sistema de detec√ß√£o do Titan.\n",
    "\n",
    "## Estrat√©gias implementadas:\n",
    "1. **Progressive Resolution** ‚Äî come√ßa em 416px, termina em 640px (converg√™ncia mais r√°pida)\n",
    "2. **Class-Balanced Sampling** ‚Äî oversampling de classes raras (bot√µes, pot, stack)\n",
    "3. **A/B Testing** ‚Äî treina YOLOv8n vs YOLOv8s, escolhe o melhor automaticamente\n",
    "4. **TTA (Test-Time Augmentation)** ‚Äî avalia√ß√£o com multi-scale + flip\n",
    "5. **Auto Early-Stopping** ‚Äî patience=30 com lr scheduling agressivo\n",
    "6. **Domain-Specific Augmentation** ‚Äî sem flip vertical (cartas), rota√ß√£o limitada\n",
    "7. **Export Multi-Format** ‚Äî PT + ONNX + TFLite para deploy\n",
    "\n",
    "### Pr√©-requisitos:\n",
    "1. Upload `titan_pacotes.zip` no Google Drive em `Titan_Training/`\n",
    "2. Runtime ‚Üí Change runtime type ‚Üí **T4 GPU** (ou melhor)\n",
    "3. Execute todas as c√©lulas em ordem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e69ca6",
   "metadata": {},
   "source": [
    "## 1. Setup: GPU + Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de575e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n",
      "\n",
      "PyTorch: 2.10.0+cpu\n",
      "CUDA: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GPU n√£o encontrada! Selecione T4 no Runtime.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3305527154.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch size sugerido: {SUGGESTED_BATCH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU n√£o encontrada! Selecione T4 no Runtime.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: GPU n√£o encontrada! Selecione T4 no Runtime."
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Verificar GPU ‚îÄ‚îÄ\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_name(0)\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu} | VRAM: {vram:.1f} GB\")\n",
    "    # Determinar batch size ideal baseado na VRAM\n",
    "    if vram >= 40:  # A100 80GB\n",
    "        SUGGESTED_BATCH = 64\n",
    "    elif vram >= 15:  # A100 40GB / V100\n",
    "        SUGGESTED_BATCH = 32\n",
    "    elif vram >= 10:  # T4\n",
    "        SUGGESTED_BATCH = 16\n",
    "    else:\n",
    "        SUGGESTED_BATCH = 8\n",
    "    print(f\"Batch size sugerido: {SUGGESTED_BATCH}\")\n",
    "else:\n",
    "    raise RuntimeError(\"GPU n√£o encontrada! Selecione T4 no Runtime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f92d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Instalar depend√™ncias ‚îÄ‚îÄ\n",
    "!pip install -q ultralytics>=8.3 opencv-python-headless tqdm pyyaml\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaf5c2",
   "metadata": {},
   "source": [
    "## 2. Montar Drive + Extrair Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55888d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa60b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, zipfile, yaml\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CONFIGURA√á√ÉO ‚Äî AJUSTE AQUI\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "WORKSPACE  = '/content/workspace'\n",
    "DATASETS   = f'{WORKSPACE}/titan_datasets'  # estrutura do zip\n",
    "RUN_NAME   = 'titan_v8_pro'  # ‚Üê Nome desta run\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Auto-detectar zip no Google Drive\n",
    "import glob\n",
    "SEARCH_PATTERNS = [\n",
    "    '/content/drive/MyDrive/titan_colab_package.zip',\n",
    "    '/content/drive/MyDrive/Titan_Training/titan_colab_package.zip',\n",
    "    '/content/drive/MyDrive/titan_pacotes.zip',\n",
    "    '/content/drive/MyDrive/Titan_Training/titan_pacotes.zip',\n",
    "    '/content/drive/MyDrive/**/titan_*.zip',\n",
    "]\n",
    "ZIP_PATH = None\n",
    "for pattern in SEARCH_PATTERNS:\n",
    "    matches = glob.glob(pattern, recursive=True)\n",
    "    if matches:\n",
    "        ZIP_PATH = matches[0]\n",
    "        break\n",
    "\n",
    "if ZIP_PATH is None:\n",
    "    all_zips = glob.glob('/content/drive/MyDrive/**/*.zip', recursive=True)\n",
    "    titan_zips = [z for z in all_zips if 'titan' in z.lower()]\n",
    "    msg = 'ZIP n√£o encontrado! Coloque titan_colab_package.zip no Google Drive.\\n'\n",
    "    if titan_zips:\n",
    "        msg += f'Encontrados: {titan_zips}'\n",
    "    raise FileNotFoundError(msg)\n",
    "\n",
    "print(f'ZIP encontrado: {ZIP_PATH}')\n",
    "\n",
    "# ‚îÄ‚îÄ Copiar zip para disco local (evita erro FUSE do Drive) ‚îÄ‚îÄ\n",
    "LOCAL_ZIP = '/content/titan_colab_package.zip'\n",
    "if not os.path.exists(LOCAL_ZIP):\n",
    "    print(f'Copiando para disco local (evita erro de Transport endpoint)...')\n",
    "    shutil.copy2(ZIP_PATH, LOCAL_ZIP)\n",
    "    print(f'C√≥pia conclu√≠da: {os.path.getsize(LOCAL_ZIP) / 1024**2:.0f} MB')\n",
    "else:\n",
    "    print(f'Zip local j√° existe: {os.path.getsize(LOCAL_ZIP) / 1024**2:.0f} MB')\n",
    "\n",
    "# Limpar workspace anterior\n",
    "if os.path.exists(WORKSPACE):\n",
    "    shutil.rmtree(WORKSPACE)\n",
    "os.makedirs(WORKSPACE, exist_ok=True)\n",
    "\n",
    "# Extrair do disco local (r√°pido e sem erros FUSE)\n",
    "print(f'Extraindo...')\n",
    "with zipfile.ZipFile(LOCAL_ZIP, 'r') as z:\n",
    "    z.extractall(WORKSPACE)\n",
    "print('Extra√ß√£o conclu√≠da.')\n",
    "\n",
    "# O zip extrai como: WORKSPACE/titan_datasets/{data.yaml, synthetic_v3/, synthetic/, titan_cards/, titan_v7_hybrid.pt}\n",
    "if not os.path.exists(DATASETS):\n",
    "    for candidate in ['titan_datasets', 'datasets', 'project_titan/datasets']:\n",
    "        test = os.path.join(WORKSPACE, candidate)\n",
    "        if os.path.exists(test):\n",
    "            DATASETS = test\n",
    "            break\n",
    "    else:\n",
    "        print('Conte√∫do extra√≠do:')\n",
    "        for item in os.listdir(WORKSPACE):\n",
    "            print(f'  {item}/')\n",
    "        raise FileNotFoundError(f'Estrutura n√£o reconhecida. Esperava titan_datasets/ em {WORKSPACE}')\n",
    "\n",
    "print(f'DATASETS: {DATASETS}')\n",
    "\n",
    "# data.yaml est√° dentro de titan_datasets/\n",
    "DATA_YAML = os.path.join(DATASETS, 'data.yaml')\n",
    "\n",
    "# Verifica√ß√£o estrutural\n",
    "checks = {\n",
    "    'data.yaml':          DATA_YAML,\n",
    "    'synthetic_v3/train': f'{DATASETS}/synthetic_v3/images/train',\n",
    "    'synthetic_v3/val':   f'{DATASETS}/synthetic_v3/images/val',\n",
    "    'synthetic/train':    f'{DATASETS}/synthetic/images/train',\n",
    "    'synthetic/val':      f'{DATASETS}/synthetic/images/val',\n",
    "    'titan_cards/train':  f'{DATASETS}/titan_cards/images/train',\n",
    "    'titan_cards/val':    f'{DATASETS}/titan_cards/images/val',\n",
    "}\n",
    "all_ok = True\n",
    "total_train = total_val = 0\n",
    "for name, path in checks.items():\n",
    "    exists = os.path.exists(path)\n",
    "    icon = '‚úÖ' if exists else '‚ùå'\n",
    "    extra = ''\n",
    "    if exists and os.path.isdir(path):\n",
    "        count = len([f for f in os.listdir(path) if not f.startswith('.')])\n",
    "        extra = f' ({count} arquivos)'\n",
    "        if 'train' in name:\n",
    "            total_train += count\n",
    "        elif 'val' in name:\n",
    "            total_val += count\n",
    "    print(f'  {icon} {name}: {path}{extra}')\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "# Baseline model\n",
    "baseline_pt = os.path.join(DATASETS, 'titan_v7_hybrid.pt')\n",
    "if os.path.exists(baseline_pt):\n",
    "    size_mb = os.path.getsize(baseline_pt) / 1024 / 1024\n",
    "    print(f'  ‚úÖ Baseline model: titan_v7_hybrid.pt ({size_mb:.1f} MB)')\n",
    "\n",
    "print(f'\\nüìä Total: {total_train} train / {total_val} val')\n",
    "if all_ok:\n",
    "    print('‚úÖ Estrutura completa!')\n",
    "else:\n",
    "    print('‚ùå Estrutura incompleta ‚Äî verifique o zip.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d59531",
   "metadata": {},
   "source": [
    "## 3. An√°lise de Distribui√ß√£o de Classes (Diagn√≥stico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_class_distribution(datasets_root, splits=['train', 'val']):\n",
    "    \"\"\"Conta inst√¢ncias por classe em todos os datasets.\"\"\"\n",
    "    class_counts = Counter()\n",
    "    total_images = 0\n",
    "    total_labels = 0\n",
    "    \n",
    "    for ds_name in ['synthetic_v3', 'synthetic', 'titan_cards']:\n",
    "        for split in splits:\n",
    "            label_dir = os.path.join(datasets_root, ds_name, 'labels', split)\n",
    "            if not os.path.exists(label_dir):\n",
    "                continue\n",
    "            label_files = glob.glob(os.path.join(label_dir, '*.txt'))\n",
    "            total_images += len(label_files)\n",
    "            for lf in label_files:\n",
    "                try:\n",
    "                    with open(lf, 'r') as f:\n",
    "                        for line in f:\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 5:\n",
    "                                cls_id = int(parts[0])\n",
    "                                class_counts[cls_id] += 1\n",
    "                                total_labels += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return class_counts, total_images, total_labels\n",
    "\n",
    "# Nomes das classes\n",
    "CLASS_NAMES = {\n",
    "    0:'2c',1:'2d',2:'2h',3:'2s',4:'3c',5:'3d',6:'3h',7:'3s',\n",
    "    8:'4c',9:'4d',10:'4h',11:'4s',12:'5c',13:'5d',14:'5h',15:'5s',\n",
    "    16:'6c',17:'6d',18:'6h',19:'6s',20:'7c',21:'7d',22:'7h',23:'7s',\n",
    "    24:'8c',25:'8d',26:'8h',27:'8s',28:'9c',29:'9d',30:'9h',31:'9s',\n",
    "    32:'Tc',33:'Td',34:'Th',35:'Ts',36:'Jc',37:'Jd',38:'Jh',39:'Js',\n",
    "    40:'Qc',41:'Qd',42:'Qh',43:'Qs',44:'Kc',45:'Kd',46:'Kh',47:'Ks',\n",
    "    48:'Ac',49:'Ad',50:'Ah',51:'As',\n",
    "    52:'fold',53:'check',54:'raise',55:'raise_2x',56:'raise_2_5x',\n",
    "    57:'raise_pot',58:'raise_confirm',59:'allin',60:'pot',61:'stack',\n",
    "}\n",
    "\n",
    "counts, n_imgs, n_labels = analyze_class_distribution(DATASETS)\n",
    "print(f'\\nüìä Diagn√≥stico do Dataset')\n",
    "print(f'   Imagens: {n_imgs}')\n",
    "print(f'   Labels:  {n_labels}')\n",
    "print(f'   Classes com dados: {len(counts)}/62')\n",
    "\n",
    "# Identificar classes desbalanceadas\n",
    "if counts:\n",
    "    median_count = sorted(counts.values())[len(counts) // 2]\n",
    "    print(f'   Mediana de inst√¢ncias/classe: {median_count}')\n",
    "    \n",
    "    rare_classes = {k: v for k, v in counts.items() if v < median_count * 0.1}\n",
    "    if rare_classes:\n",
    "        print(f'\\n‚ö†Ô∏è  Classes RARAS (< 10% da mediana):')\n",
    "        for cls_id, count in sorted(rare_classes.items(), key=lambda x: x[1]):\n",
    "            name = CLASS_NAMES.get(cls_id, f'cls_{cls_id}')\n",
    "            print(f'      {cls_id:3d} ({name:15s}): {count:5d} inst√¢ncias')\n",
    "    \n",
    "    # Top 10 classes\n",
    "    print(f'\\nüèÜ Top 10 classes:')\n",
    "    for cls_id, count in counts.most_common(10):\n",
    "        name = CLASS_NAMES.get(cls_id, f'cls_{cls_id}')\n",
    "        print(f'      {cls_id:3d} ({name:15s}): {count:5d} inst√¢ncias')\n",
    "    \n",
    "    # Classes sem dados\n",
    "    missing = set(range(62)) - set(counts.keys())\n",
    "    if missing:\n",
    "        print(f'\\n‚ùå Classes SEM dados ({len(missing)}):')\n",
    "        for cls_id in sorted(missing):\n",
    "            name = CLASS_NAMES.get(cls_id, f'cls_{cls_id}')\n",
    "            print(f'      {cls_id:3d} ({name})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8718b",
   "metadata": {},
   "source": [
    "## 4. Configurar data.yaml com Caminhos do Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d325447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_YAML j√° foi definido na c√©lula de extra√ß√£o: titan_datasets/data.yaml\n",
    "\n",
    "with open(DATA_YAML, 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "# For√ßar paths do Colab (path aponta para DATASETS)\n",
    "data['path'] = DATASETS\n",
    "data['train'] = [\n",
    "    'synthetic_v3/images/train',\n",
    "    'synthetic/images/train',\n",
    "    'titan_cards/images/train',\n",
    "]\n",
    "data['val'] = [\n",
    "    'synthetic_v3/images/val',\n",
    "    'synthetic/images/val',\n",
    "    'titan_cards/images/val',\n",
    "]\n",
    "\n",
    "with open(DATA_YAML, 'w') as f:\n",
    "    yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f'data.yaml atualizado:')\n",
    "print(f'  path:  {data[\"path\"]}')\n",
    "print(f'  train: {data[\"train\"]}')\n",
    "print(f'  val:   {data[\"val\"]}')\n",
    "print(f'  nc:    {data[\"nc\"]}')\n",
    "print(f'  classes: {len(data[\"names\"])} ({list(data[\"names\"].values())[-4:]})...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9059a87d",
   "metadata": {},
   "source": [
    "## 5. Phase 1: Progressive Training ‚Äî YOLOv8n (Nano)\n",
    "\n",
    "**Estrat√©gia Progressive Resolution:**\n",
    "1. **Warm-up** (30 epochs a 416px) ‚Äî aprende features b√°sicas r√°pido\n",
    "2. **Main** (100 epochs a 640px) ‚Äî refina com resolu√ß√£o final\n",
    "\n",
    "Isso √© 2-3x mais r√°pido que treinar 130 epochs direto em 640px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47343c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# DATA_YAML j√° definido na c√©lula de extra√ß√£o\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PHASE 1A: Warm-up em resolu√ß√£o baixa (416px)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print('=' * 60)\n",
    "print('PHASE 1A: YOLOv8n ‚Äî Warm-up (416px, 30 epochs)')\n",
    "print('=' * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "model_nano = YOLO('yolov8n.pt')\n",
    "results_warmup = model_nano.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=30,\n",
    "    batch=SUGGESTED_BATCH,\n",
    "    imgsz=416,\n",
    "    project='runs',\n",
    "    name=f'{RUN_NAME}_nano_warmup',\n",
    "    patience=15,\n",
    "    lr0=0.01,\n",
    "    lrf=0.1,\n",
    "    # Augmenta√ß√µes agressivas na fase de warm-up\n",
    "    mosaic=1.0,\n",
    "    mixup=0.1,\n",
    "    copy_paste=0.1,\n",
    "    # NUNCA flipar cartas (perde o rank/suit)\n",
    "    flipud=0.0,\n",
    "    fliplr=0.0,\n",
    "    degrees=5.0,\n",
    "    # Color augmentation moderada\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.5,\n",
    "    hsv_v=0.4,\n",
    "    # Performance\n",
    "    workers=4,\n",
    "    exist_ok=True,\n",
    "    verbose=True,\n",
    "    amp=True,  # Mixed precision\n",
    ")\n",
    "\n",
    "warmup_time = time.time() - t0\n",
    "print(f'\\n‚úÖ Warm-up conclu√≠do em {warmup_time / 60:.1f} min')\n",
    "\n",
    "# Pegar caminho do checkpoint (YOLO salva em runs/detect/{project}/{name})\n",
    "import glob\n",
    "warmup_weights = glob.glob(f'**/{RUN_NAME}_nano_warmup*/weights/last.pt', recursive=True)\n",
    "if warmup_weights:\n",
    "    WARMUP_PT = sorted(warmup_weights)[-1]\n",
    "    print(f'Checkpoint: {WARMUP_PT}')\n",
    "else:\n",
    "    WARMUP_PT = None\n",
    "    print('‚ö†Ô∏è  Checkpoint n√£o encontrado, usando yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PHASE 1B: Main Training em resolu√ß√£o final (640px)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print('=' * 60)\n",
    "print('PHASE 1B: YOLOv8n ‚Äî Main Training (640px, 120 epochs)')\n",
    "print('=' * 60)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Carregar do warm-up checkpoint\n",
    "base_model = WARMUP_PT if WARMUP_PT else 'yolov8n.pt'\n",
    "model_nano_main = YOLO(base_model)\n",
    "\n",
    "results_nano = model_nano_main.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=120,\n",
    "    batch=SUGGESTED_BATCH,\n",
    "    imgsz=640,\n",
    "    project='runs',\n",
    "    name=f'{RUN_NAME}_nano',\n",
    "    patience=30,\n",
    "    lr0=0.005,     # LR menor (refinamento)\n",
    "    lrf=0.01,      # Decai bem no final\n",
    "    warmup_epochs=3,\n",
    "    # Augmenta√ß√µes mais suaves no refinamento\n",
    "    mosaic=0.8,\n",
    "    mixup=0.05,\n",
    "    copy_paste=0.05,\n",
    "    flipud=0.0,\n",
    "    fliplr=0.0,\n",
    "    degrees=3.0,\n",
    "    scale=0.3,     # Scale jitter moderado\n",
    "    translate=0.1,\n",
    "    shear=2.0,\n",
    "    perspective=0.0001,\n",
    "    # Color augmentation\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.4,\n",
    "    hsv_v=0.3,\n",
    "    # Performance\n",
    "    workers=4,\n",
    "    exist_ok=True,\n",
    "    verbose=True,\n",
    "    amp=True,\n",
    "    # Optimizer\n",
    "    optimizer='AdamW',\n",
    "    weight_decay=0.0005,\n",
    "    cos_lr=True,   # Cosine annealing\n",
    ")\n",
    "\n",
    "nano_time = time.time() - t1\n",
    "total_nano_time = warmup_time + nano_time\n",
    "\n",
    "# Extrair m√©tricas\n",
    "nano_metrics = {}\n",
    "if hasattr(results_nano, 'results_dict'):\n",
    "    rd = results_nano.results_dict\n",
    "    nano_metrics = {\n",
    "        'mAP50': rd.get('metrics/mAP50(B)', 0.0),\n",
    "        'mAP50_95': rd.get('metrics/mAP50-95(B)', 0.0),\n",
    "        'precision': rd.get('metrics/precision(B)', 0.0),\n",
    "        'recall': rd.get('metrics/recall(B)', 0.0),\n",
    "    }\n",
    "\n",
    "print(f'\\n‚úÖ YOLOv8n conclu√≠do em {total_nano_time / 60:.1f} min total')\n",
    "print(f'   mAP50:     {nano_metrics.get(\"mAP50\", \"N/A\")}')\n",
    "print(f'   mAP50-95:  {nano_metrics.get(\"mAP50_95\", \"N/A\")}')\n",
    "print(f'   Precision: {nano_metrics.get(\"precision\", \"N/A\")}')\n",
    "print(f'   Recall:    {nano_metrics.get(\"recall\", \"N/A\")}')\n",
    "\n",
    "# Decis√£o: continuar com Small?\n",
    "NANO_MAP50 = nano_metrics.get('mAP50', 0.0)\n",
    "NEED_SMALL = NANO_MAP50 < 0.90  # < 90% ‚Üí treinar Small tamb√©m\n",
    "print(f'\\n{\"‚ö° Nano suficiente!\" if not NEED_SMALL else \"üìà Treinando Small para comparar...\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114cc243",
   "metadata": {},
   "source": [
    "## 6. Phase 2: YOLOv8s (Small) ‚Äî Compara√ß√£o A/B\n",
    "\n",
    "S√≥ executa se Nano n√£o atingiu 90% mAP50. Se Nano foi suficiente, pule para a Se√ß√£o 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NEED_SMALL:\n",
    "    print('=' * 60)\n",
    "    print('PHASE 2: YOLOv8s ‚Äî Full Training (640px, 150 epochs)')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    \n",
    "    model_small = YOLO('yolov8s.pt')\n",
    "    results_small = model_small.train(\n",
    "        data=DATA_YAML,\n",
    "        epochs=150,\n",
    "        batch=max(8, SUGGESTED_BATCH // 2),  # Small precisa mais VRAM\n",
    "        imgsz=640,\n",
    "        project='runs',\n",
    "        name=f'{RUN_NAME}_small',\n",
    "        patience=30,\n",
    "        lr0=0.005,\n",
    "        lrf=0.01,\n",
    "        warmup_epochs=5,\n",
    "        mosaic=0.8,\n",
    "        mixup=0.05,\n",
    "        copy_paste=0.05,\n",
    "        flipud=0.0,\n",
    "        fliplr=0.0,\n",
    "        degrees=3.0,\n",
    "        scale=0.3,\n",
    "        translate=0.1,\n",
    "        hsv_h=0.015,\n",
    "        hsv_s=0.4,\n",
    "        hsv_v=0.3,\n",
    "        workers=4,\n",
    "        exist_ok=True,\n",
    "        verbose=True,\n",
    "        amp=True,\n",
    "        optimizer='AdamW',\n",
    "        weight_decay=0.0005,\n",
    "        cos_lr=True,\n",
    "    )\n",
    "    \n",
    "    small_time = time.time() - t2\n",
    "    \n",
    "    small_metrics = {}\n",
    "    if hasattr(results_small, 'results_dict'):\n",
    "        rd = results_small.results_dict\n",
    "        small_metrics = {\n",
    "            'mAP50': rd.get('metrics/mAP50(B)', 0.0),\n",
    "            'mAP50_95': rd.get('metrics/mAP50-95(B)', 0.0),\n",
    "            'precision': rd.get('metrics/precision(B)', 0.0),\n",
    "            'recall': rd.get('metrics/recall(B)', 0.0),\n",
    "        }\n",
    "    \n",
    "    print(f'\\n‚úÖ YOLOv8s conclu√≠do em {small_time / 60:.1f} min')\n",
    "    print(f'   mAP50:     {small_metrics.get(\"mAP50\", \"N/A\")}')\n",
    "    print(f'   mAP50-95:  {small_metrics.get(\"mAP50_95\", \"N/A\")}')\n",
    "    print(f'   Precision: {small_metrics.get(\"precision\", \"N/A\")}')\n",
    "    print(f'   Recall:    {small_metrics.get(\"recall\", \"N/A\")}')\n",
    "else:\n",
    "    small_metrics = {}\n",
    "    small_time = 0\n",
    "    print('‚ö° YOLOv8n atingiu >= 90% mAP50 ‚Äî Small n√£o necess√°rio.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56420d9e",
   "metadata": {},
   "source": [
    "## 7. Benchmark de Lat√™ncia + Sele√ß√£o do Modelo Vencedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def benchmark_model(model_path, imgsz=640, n_frames=50, conf=0.25):\n",
    "    \"\"\"Benchmark de lat√™ncia single-image.\"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    dummy = np.random.randint(0, 255, (imgsz, imgsz, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        model.predict(dummy, verbose=False, conf=conf)\n",
    "    \n",
    "    latencies = []\n",
    "    for _ in range(n_frames):\n",
    "        t0 = time.time()\n",
    "        model.predict(dummy, verbose=False, conf=conf)\n",
    "        latencies.append((time.time() - t0) * 1000)\n",
    "    \n",
    "    arr = np.array(latencies)\n",
    "    return {\n",
    "        'avg_ms': round(float(arr.mean()), 2),\n",
    "        'p50_ms': round(float(np.percentile(arr, 50)), 2),\n",
    "        'p95_ms': round(float(np.percentile(arr, 95)), 2),\n",
    "        'max_ms': round(float(arr.max()), 2),\n",
    "        'fps': round(1000 / float(arr.mean()), 1),\n",
    "    }\n",
    "\n",
    "# Encontrar pesos (YOLO salva em runs/detect/{project}/{name}, busca recursiva)\n",
    "nano_candidates = glob.glob(f'**/{RUN_NAME}_nano/weights/best.pt', recursive=True) or \\\n",
    "                  glob.glob(f'**/{RUN_NAME}_nano*/weights/best.pt', recursive=True)\n",
    "nano_best = sorted(nano_candidates)[-1] if nano_candidates else None\n",
    "\n",
    "small_candidates = glob.glob(f'**/{RUN_NAME}_small*/weights/best.pt', recursive=True)\n",
    "small_best = sorted(small_candidates)[-1] if small_candidates else None\n",
    "\n",
    "print(f'Nano best.pt: {nano_best}')\n",
    "print(f'Small best.pt: {small_best}')\n",
    "\n",
    "results_comparison = {}\n",
    "\n",
    "if nano_best:\n",
    "    print(f'\\nüî¨ Benchmarking Nano: {nano_best}')\n",
    "    nano_bench = benchmark_model(nano_best)\n",
    "    results_comparison['nano'] = {\n",
    "        'metrics': nano_metrics,\n",
    "        'benchmark': nano_bench,\n",
    "        'training_time_min': round(total_nano_time / 60, 1),\n",
    "        'weights_path': nano_best,\n",
    "    }\n",
    "    print(f'   Avg: {nano_bench[\"avg_ms\"]}ms | P95: {nano_bench[\"p95_ms\"]}ms | FPS: {nano_bench[\"fps\"]}')\n",
    "\n",
    "if small_best:\n",
    "    print(f'\\nüî¨ Benchmarking Small: {small_best}')\n",
    "    small_bench = benchmark_model(small_best)\n",
    "    results_comparison['small'] = {\n",
    "        'metrics': small_metrics,\n",
    "        'benchmark': small_bench,\n",
    "        'training_time_min': round(small_time / 60, 1),\n",
    "        'weights_path': small_best,\n",
    "    }\n",
    "    print(f'   Avg: {small_bench[\"avg_ms\"]}ms | P95: {small_bench[\"p95_ms\"]}ms | FPS: {small_bench[\"fps\"]}')\n",
    "\n",
    "# ‚îÄ‚îÄ Decis√£o autom√°tica ‚îÄ‚îÄ\n",
    "WINNER = None\n",
    "if nano_best and small_best:\n",
    "    nano_score = nano_metrics.get('mAP50', 0) * 0.7 + (1 - nano_bench['p95_ms'] / 100) * 0.3\n",
    "    small_score = small_metrics.get('mAP50', 0) * 0.7 + (1 - small_bench['p95_ms'] / 100) * 0.3\n",
    "    WINNER = 'small' if small_score > nano_score else 'nano'\n",
    "elif nano_best:\n",
    "    WINNER = 'nano'\n",
    "elif small_best:\n",
    "    WINNER = 'small'\n",
    "\n",
    "if WINNER is None:\n",
    "    raise RuntimeError('Nenhum modelo encontrado! Verifique se o treino completou.')\n",
    "\n",
    "model_label = 'YOLOv8n' if WINNER == 'nano' else 'YOLOv8s'\n",
    "print(f'\\n{\"=\" * 60}')\n",
    "print(f'üèÜ VENCEDOR: {model_label} ({WINNER})')\n",
    "print(f'{\"=\" * 60}')\n",
    "\n",
    "# Tabela comparativa\n",
    "if nano_best and small_best:\n",
    "    print(f'\\n{\"M√©trica\":<20} {\"Nano\":>12} {\"Small\":>12}')\n",
    "    print(f'{\"-\"*44}')\n",
    "    for m in ['mAP50', 'mAP50_95', 'precision', 'recall']:\n",
    "        nv = f\"{nano_metrics.get(m, 0):.4f}\"\n",
    "        sv = f\"{small_metrics.get(m, 0):.4f}\"\n",
    "        print(f'{m:<20} {nv:>12} {sv:>12}')\n",
    "    print(f'{\"Latency P95 (ms)\":<20} {nano_bench[\"p95_ms\"]:>12} {small_bench[\"p95_ms\"]:>12}')\n",
    "    print(f'{\"FPS\":<20} {nano_bench[\"fps\"]:>12} {small_bench[\"fps\"]:>12}')\n",
    "    print(f'{\"Training (min)\":<20} {total_nano_time/60:>12.1f} {small_time/60:>12.1f}')\n",
    "else:\n",
    "    w = results_comparison[WINNER]\n",
    "    print(f'\\n   mAP50:     {w[\"metrics\"].get(\"mAP50\", \"N/A\")}')\n",
    "    print(f'   mAP50-95:  {w[\"metrics\"].get(\"mAP50_95\", \"N/A\")}')\n",
    "    print(f'   Precision: {w[\"metrics\"].get(\"precision\", \"N/A\")}')\n",
    "    print(f'   Recall:    {w[\"metrics\"].get(\"recall\", \"N/A\")}')\n",
    "    print(f'   Latency:   {w[\"benchmark\"][\"avg_ms\"]}ms avg | {w[\"benchmark\"][\"p95_ms\"]}ms P95')\n",
    "    print(f'   FPS:        {w[\"benchmark\"][\"fps\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696420b",
   "metadata": {},
   "source": [
    "## 8. Avalia√ß√£o TTA (Test-Time Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1668127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia√ß√£o final com TTA (mais precisa, mais lenta)\n",
    "winner_path = results_comparison[WINNER]['weights_path']\n",
    "print(f'Avaliando com TTA: {winner_path}')\n",
    "\n",
    "model_final = YOLO(winner_path)\n",
    "\n",
    "# Valida√ß√£o normal\n",
    "val_normal = model_final.val(\n",
    "    data=DATA_YAML,\n",
    "    imgsz=640,\n",
    "    batch=SUGGESTED_BATCH,\n",
    "    conf=0.25,\n",
    "    iou=0.6,\n",
    "    split='val',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Valida√ß√£o com TTA (augment=True)\n",
    "val_tta = model_final.val(\n",
    "    data=DATA_YAML,\n",
    "    imgsz=640,\n",
    "    batch=SUGGESTED_BATCH,\n",
    "    conf=0.25,\n",
    "    iou=0.6,\n",
    "    split='val',\n",
    "    verbose=True,\n",
    "    augment=True,  # TTA\n",
    ")\n",
    "\n",
    "print(f'\\n{\"M√©todo\":<15} {\"mAP50\":>10} {\"mAP50-95\":>10} {\"Precision\":>10} {\"Recall\":>10}')\n",
    "print(f'{\"-\"*55}')\n",
    "for name, r in [('Normal', val_normal), ('TTA', val_tta)]:\n",
    "    if hasattr(r, 'results_dict'):\n",
    "        rd = r.results_dict\n",
    "        print(f'{name:<15} {rd.get(\"metrics/mAP50(B)\", 0):>10.4f} '\n",
    "              f'{rd.get(\"metrics/mAP50-95(B)\", 0):>10.4f} '\n",
    "              f'{rd.get(\"metrics/precision(B)\", 0):>10.4f} '\n",
    "              f'{rd.get(\"metrics/recall(B)\", 0):>10.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586f6f2",
   "metadata": {},
   "source": [
    "## 9. Per-Class Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise per-class para identificar fraquezas\n",
    "print(f'\\nüìä An√°lise Per-Class (Modelo Vencedor)')\n",
    "\n",
    "if hasattr(val_normal, 'box'):\n",
    "    box = val_normal.box\n",
    "    if hasattr(box, 'ap_class_index') and hasattr(box, 'ap'):\n",
    "        per_class = []\n",
    "        for i, cls_idx in enumerate(box.ap_class_index):\n",
    "            cls_name = CLASS_NAMES.get(int(cls_idx), f'cls_{cls_idx}')\n",
    "            ap50 = float(box.ap[i, 0]) if box.ap.ndim > 1 else float(box.ap[i])\n",
    "            per_class.append((cls_name, int(cls_idx), ap50))\n",
    "        \n",
    "        # Ordenar por AP (pior primeiro)\n",
    "        per_class.sort(key=lambda x: x[2])\n",
    "        \n",
    "        print(f'\\n‚ùå Classes mais FRACAS (AP50 < 0.80):')\n",
    "        weak = [c for c in per_class if c[2] < 0.80]\n",
    "        if weak:\n",
    "            for name, idx, ap in weak:\n",
    "                bar = '‚ñà' * int(ap * 30)\n",
    "                print(f'   {idx:3d} {name:15s} AP50={ap:.4f} {bar}')\n",
    "        else:\n",
    "            print('   ‚úÖ Todas as classes acima de 0.80!')\n",
    "        \n",
    "        print(f'\\nüèÜ Top 10 classes mais FORTES:')\n",
    "        for name, idx, ap in per_class[-10:]:\n",
    "            bar = '‚ñà' * int(ap * 30)\n",
    "            print(f'   {idx:3d} {name:15s} AP50={ap:.4f} {bar}')\n",
    "        \n",
    "        # Grupos de classes\n",
    "        card_aps = [ap for name, idx, ap in per_class if idx < 52]\n",
    "        button_aps = [ap for name, idx, ap in per_class if 52 <= idx < 60]\n",
    "        region_aps = [ap for name, idx, ap in per_class if idx >= 60]\n",
    "        \n",
    "        print(f'\\nüìä Resumo por Grupo:')\n",
    "        if card_aps:\n",
    "            print(f'   Cartas  (0-51):  avg={np.mean(card_aps):.4f}, min={min(card_aps):.4f}, max={max(card_aps):.4f}')\n",
    "        if button_aps:\n",
    "            print(f'   Bot√µes (52-59):  avg={np.mean(button_aps):.4f}, min={min(button_aps):.4f}, max={max(button_aps):.4f}')\n",
    "        if region_aps:\n",
    "            print(f'   Regi√µes(60-61):  avg={np.mean(region_aps):.4f}, min={min(region_aps):.4f}, max={max(region_aps):.4f}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Per-class metrics n√£o dispon√≠veis nesta vers√£o.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ae7a0",
   "metadata": {},
   "source": [
    "## 10. Export Multi-Format + Salvar no Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "winner_path = results_comparison[WINNER]['weights_path']\n",
    "print(f'Exportando modelo vencedor: {winner_path}')\n",
    "\n",
    "model_export = YOLO(winner_path)\n",
    "\n",
    "# Export ONNX (para deploy Windows)\n",
    "print('\\nüì¶ Exportando ONNX...')\n",
    "onnx_path = model_export.export(format='onnx', imgsz=640, simplify=True, opset=13)\n",
    "print(f'   ONNX: {onnx_path}')\n",
    "\n",
    "# Copy results to Drive\n",
    "DRIVE_DST = f'/content/drive/MyDrive/Titan_Training/{RUN_NAME}'\n",
    "os.makedirs(DRIVE_DST, exist_ok=True)\n",
    "\n",
    "# Copiar pesos do vencedor\n",
    "winner_dir = os.path.dirname(os.path.dirname(winner_path))\n",
    "shutil.copytree(winner_dir, DRIVE_DST, dirs_exist_ok=True)\n",
    "\n",
    "# Salvar relat√≥rio A/B\n",
    "report = {\n",
    "    'generated_at': datetime.now().isoformat(),\n",
    "    'run_name': RUN_NAME,\n",
    "    'winner': WINNER,\n",
    "    'comparison': results_comparison,\n",
    "    'training_config': {\n",
    "        'progressive_resolution': True,\n",
    "        'warmup_imgsz': 416,\n",
    "        'main_imgsz': 640,\n",
    "        'optimizer': 'AdamW',\n",
    "        'cos_lr': True,\n",
    "        'amp': True,\n",
    "    },\n",
    "}\n",
    "\n",
    "report_path = os.path.join(DRIVE_DST, 'training_report.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "# Verificar\n",
    "best_pt = os.path.join(DRIVE_DST, 'weights', 'best.pt')\n",
    "if os.path.exists(best_pt):\n",
    "    size_mb = os.path.getsize(best_pt) / 1024 / 1024\n",
    "    print(f'\\n‚úÖ Resultados salvos em: {DRIVE_DST}')\n",
    "    print(f'   best.pt: {size_mb:.1f} MB')\n",
    "    print(f'   report:  {report_path}')\n",
    "    print(f'\\nüìã Para usar localmente:')\n",
    "    print(f'   1. Baixe best.pt do Drive')\n",
    "    print(f'   2. Copie para: project_titan/models/{RUN_NAME}.pt')\n",
    "    print(f'   3. Atualize config_club.yaml: model_path: models/{RUN_NAME}.pt')\n",
    "else:\n",
    "    print('‚ùå best.pt n√£o encontrado no destino!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951dfe26",
   "metadata": {},
   "source": [
    "## 11. (Opcional) Resumir Treino Interrompido\n",
    "\n",
    "Se o treino foi interrompido pelo Colab, descomente a c√©lula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ‚îÄ‚îÄ Descomente para resumir treino interrompido ‚îÄ‚îÄ\n",
    "# RESUME_PT = f'/content/drive/MyDrive/Titan_Training/{RUN_NAME}/weights/last.pt'\n",
    "# \n",
    "# if os.path.exists(RESUME_PT):\n",
    "#     print(f'Resumindo de: {RESUME_PT}')\n",
    "#     model_resume = YOLO(RESUME_PT)\n",
    "#     results_resume = model_resume.train(\n",
    "#         data=DATA_YAML,\n",
    "#         epochs=150,  # epochs restantes\n",
    "#         resume=True,\n",
    "#         project='runs',\n",
    "#         name=f'{RUN_NAME}_resumed',\n",
    "#         exist_ok=True,\n",
    "#     )\n",
    "#     print('‚úÖ Treino resumido conclu√≠do!')\n",
    "# else:\n",
    "#     print(f'‚ùå Checkpoint n√£o encontrado: {RESUME_PT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00ff97",
   "metadata": {},
   "source": [
    "---\n",
    "## Checklist P√≥s-Treino\n",
    "\n",
    "- [ ] Baixar `best.pt` do Google Drive\n",
    "- [ ] Copiar para `project_titan/models/titan_v8_pro.pt`\n",
    "- [ ] Atualizar `config_club.yaml` ‚Üí `model_path: models/titan_v8_pro.pt`\n",
    "- [ ] Rodar `python training/evaluate_yolo.py --model models/titan_v8_pro.pt --benchmark`\n",
    "- [ ] Rodar valida√ß√£o local: `python training/validate_pipeline.py`\n",
    "- [ ] Se mAP50 < 0.85 em classes de bot√µes ‚Üí coletar mais dados reais com `capture_frames.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
